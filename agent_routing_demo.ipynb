{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6344abb",
   "metadata": {},
   "source": [
    "# Agent Routing: Hallucinated Scores vs. True Math\n",
    "\n",
    "This notebook demonstrates two ways to get \"confidence\" from an LLM regarding agent routing decisions.\n",
    "\n",
    "1.  **The Naive Way (BAD):** Asking the model to output a number (0-100). This is a hallucination.\n",
    "2.  **The Scientific Way (GOOD):** Using `logprobs` to measure the statistical probability of the next token.\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db578c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional, Tuple, Literal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load env vars (expects OPENAI_API_KEY in your .env)\n",
    "load_dotenv()\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not set. Create a .env with your key before running this notebook.\"\n",
    "    )\n",
    "\n",
    "# --- CONFIG: AGENT DEFINITIONS ---\n",
    "AGENT_CARDS: List[Dict[str, str]] = [\n",
    "    {\n",
    "        \"name\": \"TravelBuddy\",\n",
    "        \"description\": \"Plan quick city trips with budgets, flights, and local tips.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CodeFixer\",\n",
    "        \"description\": \"Debug and refactor small Python or JavaScript snippets.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"HealthNote\",\n",
    "        \"description\": \"Summarize lifestyle and nutrition questions into simple advice.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"BizPitch\",\n",
    "        \"description\": \"Draft short startup pitches and positioning statements.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"DataScout\",\n",
    "        \"description\": \"Explain CSV/Excel columns and basic data cleaning steps.\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b5ffd",
   "metadata": {},
   "source": [
    "## Part 1: The \"Trap\" (Naive Confidence Scores)\n",
    "\n",
    "This is what managers usually ask for: *\"Just ask the AI how sure it is!\"*\n",
    "\n",
    "**Why this is dangerous:**\n",
    "When the LLM outputs \"Confidence: 95%\", it is **not** doing math. It is predicting the text characters \"9\" and \"5\" because they look plausible in the context of the prompt. It is a simulation of confidence, not a measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a74565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveConfidenceOutput(BaseModel):\n",
    "    decision: Literal[\"YES\", \"NO\"]\n",
    "    confidence_score: int = Field(description=\"Confidence level from 0 to 100\")\n",
    "    reasoning: str\n",
    "\n",
    "naive_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(NaiveConfidenceOutput)\n",
    "\n",
    "def get_naive_score(query: str, card: Dict[str, str]):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a router. Check if the agent matches the query. Provide a confidence score (0-100) reflecting how sure you are.\"),\n",
    "        (\"user\", \"Query: {query}\\nAgent: {name} ({description})\")\n",
    "    ])\n",
    "    chain = prompt | naive_llm\n",
    "    return chain.invoke({\"query\": query, \"name\": card[\"name\"], \"description\": card[\"description\"]})\n",
    "\n",
    "test_query = \"Plan a weekend trip to Porto.\"\n",
    "test_agent = AGENT_CARDS[0] \n",
    "\n",
    "result = get_naive_score(test_query, test_agent)\n",
    "\n",
    "print(f\"QUERY: {test_query}\")\n",
    "print(f\"AGENT: {test_agent['name']}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"DECISION: {result.decision}\")\n",
    "print(f\"NAIVE SCORE: {result.confidence_score}%\")\n",
    "print(f\"REASON: {result.reasoning}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"WARNING: This number (98/99/100) is just text generated by the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007517e1",
   "metadata": {},
   "source": [
    "## Part 2: The Solution (Logprobs)\n",
    "\n",
    "Instead of asking for a number, we look at the **raw token probabilities**.\n",
    "\n",
    "We ask the model to predict **\"YES\"** or **\"NO\"**.\n",
    "We enable `logprobs=True` to see the statistical likelihood of those specific tokens before the model even makes a final choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e2a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_base = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.3,\n",
    "    logprobs=True,\n",
    "    top_logprobs=10,\n",
    "    max_tokens=64,\n",
    ")\n",
    "\n",
    "Answer = Literal[\"YES\", \"NO\"]\n",
    "\n",
    "class RouterLLMOutput(BaseModel):\n",
    "    answer: Answer = Field(description=\"Either 'YES' or 'NO'\")\n",
    "    reasoning: str = Field(description=\"One short English sentence\")\n",
    "\n",
    "router_llm = llm_base.with_structured_output(\n",
    "    RouterLLMOutput,\n",
    "    include_raw=True,\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a neutral binary classifier for agent routing.\n",
    "You receive a user query and ONE agent card.\n",
    "Decide if this agent is suitable.\n",
    "Rules:\n",
    "- Set \"answer\" to \"YES\" or \"NO\" (uppercase).\n",
    "- \"reasoning\" must be one short sentence in English.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        (\"user\", \"User Query:\\n{query}\\n\\nAgent Card:\\n{name}: {description}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e8d75",
   "metadata": {},
   "source": [
    "### Helper Functions for Math\n",
    "We need to extract the tokens, normalize them (e.g., \" yes\" == \"YES\"), and sum up their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508ea010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YesNoStat(BaseModel):\n",
    "    token: str\n",
    "    logprob: float\n",
    "    prob: float\n",
    "\n",
    "class AgentDecision(BaseModel):\n",
    "    agent_name: str\n",
    "    answer: Answer\n",
    "    reasoning: str\n",
    "    prob_yes: Optional[float] = None\n",
    "    prob_no: Optional[float] = None\n",
    "    raw_candidates: List[Tuple[str, float]] = Field(default_factory=list)\n",
    "\n",
    "def _normalize_label_token(tok: str) -> str:\n",
    "    return tok.strip().strip(\"\\\"'\").upper()\n",
    "\n",
    "def extract_yes_no_enum_stats(logprobs: Dict[str, Any]) -> Tuple[float, float, List[YesNoStat], List[Tuple[str, float]]]:\n",
    "    content = logprobs[\"content\"]\n",
    "    decision_candidates: List[Tuple[str, float]] | None = None\n",
    "\n",
    "    for step in content:\n",
    "        candidates = [(step[\"token\"], step[\"logprob\"])]\n",
    "        for alt in step[\"top_logprobs\"]:\n",
    "            candidates.append((alt[\"token\"], alt[\"logprob\"]))\n",
    "\n",
    "        if any(_normalize_label_token(t) in (\"YES\", \"NO\") for t, _ in candidates):\n",
    "            decision_candidates = candidates\n",
    "            break\n",
    "\n",
    "    if decision_candidates is None:\n",
    "        return 0.0, 0.0, [], []\n",
    "\n",
    "    best_lp_yes = None\n",
    "    best_lp_no = None\n",
    "\n",
    "    for tok_raw, lp_raw in decision_candidates:\n",
    "        norm = _normalize_label_token(tok_raw)\n",
    "        if norm == \"YES\":\n",
    "            if best_lp_yes is None or lp_raw > best_lp_yes:\n",
    "                best_lp_yes = lp_raw\n",
    "        elif norm == \"NO\":\n",
    "            if best_lp_no is None or lp_raw > best_lp_no:\n",
    "                best_lp_no = lp_raw\n",
    "\n",
    "    logits = []\n",
    "    if best_lp_yes is not None:\n",
    "        logits.append((\"YES\", best_lp_yes))\n",
    "    if best_lp_no is not None:\n",
    "        logits.append((\"NO\", best_lp_no))\n",
    "\n",
    "    if not logits:\n",
    "        return 0.0, 0.0, [], decision_candidates\n",
    "\n",
    "    max_lp = max(lp for _, lp in logits)\n",
    "    exps = [(label, math.exp(lp - max_lp)) for label, lp in logits]\n",
    "    denom = sum(v for _, v in exps)\n",
    "\n",
    "    p_yes = 0.0\n",
    "    p_no = 0.0\n",
    "    stats = []\n",
    "\n",
    "    for label, val in exps:\n",
    "        prob = val / denom\n",
    "        lp_label = best_lp_yes if label == \"YES\" else best_lp_no\n",
    "        stats.append(YesNoStat(token=label, logprob=lp_label, prob=prob))\n",
    "        if label == \"YES\":\n",
    "            p_yes = prob\n",
    "        else:\n",
    "            p_no = prob\n",
    "\n",
    "    return p_yes, p_no, stats, decision_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739345db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query_with_logprobs(query: str):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    decisions = []\n",
    "\n",
    "    for card in AGENT_CARDS:\n",
    "        messages = prompt.format_messages(query=query, name=card[\"name\"], description=card[\"description\"])\n",
    "        result = router_llm.invoke(messages)\n",
    "\n",
    "        logprobs = result[\"raw\"].response_metadata[\"logprobs\"]\n",
    "        p_yes, p_no, _, raw_candidates = extract_yes_no_enum_stats(logprobs)\n",
    "\n",
    "        decisions.append(AgentDecision(\n",
    "            agent_name=card[\"name\"],\n",
    "            answer=result[\"parsed\"].answer,\n",
    "            reasoning=result[\"parsed\"].reasoning,\n",
    "            prob_yes=p_yes, prob_no=p_no,\n",
    "            raw_candidates=raw_candidates\n",
    "        ))\n",
    "\n",
    "    decisions.sort(key=lambda x: x.prob_yes or 0.0, reverse=True)\n",
    "\n",
    "    table_data = [[d.agent_name, d.answer, f\"{d.prob_yes:.4f}\", f\"{d.prob_no:.4f}\", d.reasoning] for d in decisions]\n",
    "    print(tabulate(table_data, headers=[\"Agent\", \"Ans\", \"p(YES)\", \"p(NO)\", \"Reason\"], tablefmt=\"simple\"))\n",
    "\n",
    "    if decisions:\n",
    "        top = decisions[0]\n",
    "        print(f\"\\n>>> DEEP DIVE: Token probabilities for Agent '{top.agent_name}'\")\n",
    "\n",
    "        raw_rows = []\n",
    "        sorted_candidates = sorted(top.raw_candidates, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        total_vis = 0.0\n",
    "        for tok, lp in sorted_candidates:\n",
    "            prob = math.exp(lp) * 100\n",
    "            total_vis += prob\n",
    "            clean = _normalize_label_token(tok)\n",
    "            cat = \"YES bucket\" if clean == \"YES\" else (\"NO bucket\" if clean == \"NO\" else \"Ignored\")\n",
    "            raw_rows.append([repr(tok), f\"{lp:.4f}\", f\"{prob:.2f}%\", cat])\n",
    "\n",
    "        print(tabulate(raw_rows, headers=[\"Token\", \"Logprob\", \"Prob(%)\", \"Bucket\"], tablefmt=\"github\"))\n",
    "        print(f\"... remaining {100-total_vis:.4f}% is in the tail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72b26d",
   "metadata": {},
   "source": [
    "## Part 3: Running the Deep Dive\n",
    "Now we see the \"Science\" behind the score. Notice how multiple tokens (e.g. `\"YES\"` vs `\"Yes\"`) might appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd19b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Plan a weekend trip to Porto with a $600 budget.\",\n",
    "    \"Is a tomato a vegetable? Just give me the fact.\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    route_query_with_logprobs(q)\n",
    "    print(\"\\n\" * 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
